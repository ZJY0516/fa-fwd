diff --git a/hopper/setup.py b/hopper/setup.py
index 95729ed..f3c5fa3 100644
--- a/hopper/setup.py
+++ b/hopper/setup.py
@@ -26,14 +26,14 @@ from torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtensio
 
 
 # with open("../README.md", "r", encoding="utf-8") as fh:
-with open("../README.md", "r", encoding="utf-8") as fh:
+with open("../../README.md", "r", encoding="utf-8") as fh:
     long_description = fh.read()
 
 
 # ninja build does not work unless include_dirs are abs path
 this_dir = os.path.dirname(os.path.abspath(__file__))
 
-PACKAGE_NAME = "flash_attn_3"
+PACKAGE_NAME = "fa3_fwd"
 
 BASE_WHEEL_URL = "https://github.com/Dao-AILab/flash-attention/releases/download/{tag_name}/{wheel_name}"
 
@@ -108,7 +108,7 @@ def create_build_config_file():
         }
     }
 
-    with open("flash_attn_config.py", "w") as f:
+    with open("fa3_fwd_config.py", "w") as f:
         f.write("# Auto-generated by flash attention 3 setup.py\n")
         f.write(f"CONFIG = {repr(CONFIG)}\n")
         f.write("\n")
@@ -597,9 +597,9 @@ if not SKIP_CUDA_BUILD:
         "--use_fast_math",
         # "--keep",
         # "--ptxas-options=--verbose,--register-usage-level=5,--warn-on-local-memory-usage",  # printing out number of registers
-        "--resource-usage",  # printing out number of registers
+        # "--resource-usage",  # printing out number of registers
         # f"--split-compile={os.getenv('NVCC_THREADS', '4')}",  # split-compile is faster
-        "-lineinfo",  # TODO: disable this for release to reduce binary size
+        # "-lineinfo",  # TODO: disable this for release to reduce binary size
         "-DCUTE_SM90_EXTENDED_MMA_SHAPES_ENABLED",  # Necessary for the WGMMA shapes that we use
         "-DCUTLASS_ENABLE_GDC_FOR_SM90",  # For PDL
         "-DCUTLASS_DEBUG_TRACE_LEVEL=0",  # Can toggle for debugging
@@ -713,8 +713,8 @@ setup(
             "benchmarks",
         )
     ),
-    py_modules=["flash_attn_interface", "flash_attn_config"],
-    description="FlashAttention-3",
+    py_modules=["fa3_fwd_interface", "fa3_fwd_config"],
+    description="FlashAttention-3 forward",
     long_description=long_description,
     long_description_content_type="text/markdown",
     classifiers=[
